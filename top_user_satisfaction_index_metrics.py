# -*- coding: utf-8 -*-
"""Top User Satisfaction Index Metrics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B9WiWbHrKvqWofVU3zfHloeLjbgkvay5
"""

import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, accuracy_score
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from matplotlib import pyplot
from matplotlib import pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.inspection import permutation_importance
from sklearn.decomposition import PCA
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn import svm
from sklearn.svm import LinearSVC
from xgboost import XGBClassifier
import sklearn
from scipy import stats as st

def plot_coefficients(classifier, feature_names, top_features=20):
  coef = classifier.coef_.ravel()
  top_positive_coefficients = np.argsort(coef)[-top_features:]
  top_negative_coefficients = np.argsort(coef)[:top_features]
  top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])
  # create plot
  plt.figure(figsize=(15, 5))
  colors = ['red' if c < 0 else 'blue' for c in coef[top_coefficients]]
  plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)
  feature_names = np.array(feature_names)
  plt.xticks(np.arange(1, 1 + 2 * top_features), feature_names[top_coefficients], rotation=60, ha='right')
  plt.show()

from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)

#the workbook object created above, wb, allows us to specify which sheet we want by using the worksheet() function. 
#This simply allows you to specify the sheet you want by just calling the name of the sheet.
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/FYP data set/fyp_data_2.csv')

df.head()

df.corr()

df.describe()

perc = [0.01,0.05,0.1,0.25,0.5,0.75,0.9,0.95,0.99]
df.describe(percentiles =perc)

plt.figure(figsize = (20,20))

ax = sns.heatmap(df.corr(), annot=True, fmt='.0%')

#df  = df.iloc[: , :8]

df.nunique()

"""Apply t-test to find out how significant is the relationship of each explanatory variable with target variable """

for feature_name in df.columns:
    a = df.loc[df['Satisfied']==1, feature_name]
    b = df.loc[df['Satisfied']==0, feature_name]
    print('t-test results for: ', feature_name)
    print(st.ttest_ind(a,b))

X = df.drop(['Satisfied','USER'], axis=1) #contains all the Independent variables
y = df['Satisfied'] #contains the target variable
print(f'X shape: {X.shape} | y shape: {y.shape} ')

"""Normalize Data """

for feature_name in df.columns:
    df[feature_name] = df[feature_name] / df[feature_name].std()

df

LR_Model = LogisticRegression(random_state=0, max_iter=5000)
scores = cross_val_score(LR_Model, X, y, cv=20, scoring = 'f1')

print(scores.mean())

DT_Model = tree.DecisionTreeClassifier(random_state=0)
scores = cross_val_score(DT_Model, X, y, cv=20, scoring = 'f1')
print(scores.mean())

RF_Model = RandomForestClassifier(random_state=0,n_estimators=300)
scores = cross_val_score(RF_Model, X, y, cv=20, scoring = 'f1')
print(scores.mean())

LinearSVC_Model = LinearSVC(max_iter=5000,random_state=0)
scores = cross_val_score(LinearSVC_Model, X, y, cv=20, scoring = 'f1')

print(scores.mean())

LR_Model.fit(X, y)
DT_Model.fit(X, y)
RF_Model.fit(X,y)
LinearSVC_Model.fit(X, y)

import math 

w = LR_Model.coef_[0]
print(w)

fea_imp = pd.DataFrame({'imp': DT_Model.feature_importances_, 'col': X.columns})
fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]
fea_imp.plot(kind='barh', x='col', y='imp', figsize=(10, 7), legend=None)
plt.title('Decision Tree - Feature Importance')
plt.ylabel('Features')
plt.xlabel('Importance')

fea_imp = pd.DataFrame({'imp': RF_Model.feature_importances_, 'col': X.columns})
fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]
fea_imp.plot(kind='barh', x='col', y='imp', figsize=(10, 7), legend=None)
plt.title('Random Forest - Feature Importance')
plt.ylabel('Features')
plt.xlabel('Importance')

tree.plot_tree(DT_Model)

plot_coefficients(LinearSVC_Model, X.columns)

from matplotlib.pyplot import figure


feature_importance = pd.DataFrame(X.columns, columns = ["feature"])
feature_importance["importance"] = pow(math.e, w)
feature_importance = feature_importance.sort_values(by = ["importance"], ascending=True)
plt.rcParams["figure.figsize"] = (100,50)
plt.rcParams.update({'font.size': 50})
ax = feature_importance.plot.barh(x='feature', y='importance',width=0.7)
plt.show()